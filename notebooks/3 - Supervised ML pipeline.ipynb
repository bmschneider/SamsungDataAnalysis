{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impressive-robert",
   "metadata": {},
   "source": [
    "# Supervised learning pipeline\n",
    "\n",
    "This notebook contains a supervised approach to predict the activities in the data. We consider the problem of predicting the activity for the time stamp of the same record input into the algorithm; a cross-sectional approach. We choose a particular type of model, `LogisticRegression`, in this notebook.\n",
    "\n",
    "We build in a few conveniences into the code here. Specifically,\n",
    "* Using a `Pipeline` to combine data transformation and model\n",
    "* Using a `PCA` model to reduce the dimensionality of the data\n",
    "* Using a grouped cross-validation to train\n",
    "* Using a `GridSearchCV` to tune hyperparameters\n",
    "\n",
    "We finally test the model on the test data set, as defined by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-portal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "roman-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-context",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "Load the feature data and then prepare the train / test objects. We use the author's definition of training and testing datasets. The training data will also include validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "engaging-victim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 564)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities = load_activity_names(); activities\n",
    "features_df = load_feature_data() \\\n",
    "    .merge(activities) \\\n",
    "    .drop('activity_id', axis=1) \\\n",
    "    .sort_values(['subject_id', 'time_window_s']) \\\n",
    "    .reset_index(drop=True)\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-burning",
   "metadata": {},
   "source": [
    "We only input the data features into the model, so we need to skip subject, time, and activity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acknowledged-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features_df.drop(['subject_id', 'time_window_s', 'activity_name'], axis=1)\n",
    "y_train = features_df.activity_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "burning-report",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2947, 564)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test_df = load_feature_data('test') \\\n",
    "    .merge(activities) \\\n",
    "    .drop('activity_id', axis=1) \\\n",
    "    .sort_values(['subject_id', 'time_window_s']) \\\n",
    "    .reset_index(drop=True)\n",
    "features_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-residence",
   "metadata": {},
   "source": [
    "## Model set-up and fitting / searching\n",
    "\n",
    "Since we have many features, reducing the dimensionality is recommended. Some notes:\n",
    "* We use a `Pipeline` to facilitate fitted parameters of scaling, dimension reducing via `PCA`, and the classifier\n",
    "* We use a grouped cross-validation strategy based on subjects\n",
    "* The `GridSearchCV` will try all combinations of hyperparameters with brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "figured-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-joining",
   "metadata": {},
   "source": [
    "### Model training / searching setup\n",
    "\n",
    "When we cross-validate, we don't really want an individual's data points split across train and validation. This is leakage of the unique behavior of that individual from validation to train data. Therefore, we use a `GroupKFold` object which can `.split` the data by an index, and we can choose the `subject_id` for this. We'll define `subjects_train` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "urban-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_group = GroupKFold(n_splits=5)\n",
    "subjects_train = features_df.subject_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-cleaner",
   "metadata": {},
   "source": [
    "We'll try a range of parameters for the `C` parameter of `LogisticRegression` and `n_components` for the `PCA` model. This is flexible in that one can include other parameters with ranges, but it will only support one model object at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "legitimate-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_dict = {'lr__C': [0.001, 0.01, 0.1, 1., 10., 100., 1000.],\n",
    "                   'pca__n_components': [150, 200, 250]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "racial-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe = Pipeline([('ss', StandardScaler()),\n",
    "                       ('pca', PCA()),\n",
    "                       ('lr', LogisticRegression(solver='liblinear'))])\n",
    "search_pipe = GridSearchCV(estimator=model_pipe,\n",
    "                           param_grid=hyperparam_dict,\n",
    "                           cv = cv_group.split(X_train, y_train, groups=subjects_train),\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-officer",
   "metadata": {},
   "source": [
    "### Model training / searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-airfare",
   "metadata": {},
   "source": [
    "The `GridSearchCV` works like any model `Pipeline` or an individual model (or \"estimator\") in that it has a `.fit` method to \"train\" the model (performing the model search), and a `.predict` method to apply the chosen model to data.\n",
    "\n",
    "To get the `Pipeline` it chose, it provides a `.best_estimator_` attribute once it's trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "straight-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ss', StandardScaler()), ('pca', PCA(n_components=250)),\n",
       "                ('lr', LogisticRegression(solver='liblinear'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_pipe_fit = search_pipe.fit(X_train, y_train)\n",
    "search_pipe_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-appreciation",
   "metadata": {},
   "source": [
    "Separately, the hyperparameters can be found using the `.best_params_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cross-twins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 1.0, 'pca__n_components': 250}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_pipe_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-better",
   "metadata": {},
   "source": [
    "### Evaluate on the test data\n",
    "\n",
    "We evaluate the model on the test data that was defined by the authors. `sklearn` provides a convenience function `accuracy_score` to compute the accuracy, and `classification_report` to compute precision, recall, F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "satisfactory-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = search_pipe_fit \\\n",
    "    .predict(features_test_df.drop(['subject_id', 'time_window_s', 'activity_name'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "capital-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            LAYING       0.98      1.00      0.99       525\n",
      "           SITTING       0.87      0.95      0.91       452\n",
      "          STANDING       0.96      0.88      0.92       584\n",
      "           WALKING       1.00      0.96      0.98       515\n",
      "WALKING_DOWNSTAIRS       0.98      1.00      0.99       414\n",
      "  WALKING_UPSTAIRS       0.96      0.99      0.97       457\n",
      "\n",
      "          accuracy                           0.96      2947\n",
      "         macro avg       0.96      0.96      0.96      2947\n",
      "      weighted avg       0.96      0.96      0.96      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(classification_report(y_test_hat, features_test_df.activity_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-macro",
   "metadata": {},
   "source": [
    "When we cross-tabulate the actual labels with the classified ones, we see a pretty diagonal matrix. Indeed, laying has been 100% correct.\n",
    "\n",
    "Here we will want to validate if the errors made are acceptable. For example, errors for walking downstairs are either walking or walking upstairs. It may be important to continue tuning parameters such that this activity is never (or less commonly) misclassified as walking upstairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "physical-paste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classified</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>STANDING</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LAYING</th>\n",
       "      <td>524</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITTING</th>\n",
       "      <td>1</td>\n",
       "      <td>429</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STANDING</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALKING</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>413</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Classified          LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
       "True                                                                         \n",
       "LAYING                 524        0        13        0                   0   \n",
       "SITTING                  1      429        59        0                   0   \n",
       "STANDING                 0       20       512        0                   0   \n",
       "WALKING                  0        2         0      494                   0   \n",
       "WALKING_DOWNSTAIRS       0        1         0        3                 413   \n",
       "WALKING_UPSTAIRS         0        0         0       18                   1   \n",
       "\n",
       "Classified          WALKING_UPSTAIRS  \n",
       "True                                  \n",
       "LAYING                             0  \n",
       "SITTING                            2  \n",
       "STANDING                           0  \n",
       "WALKING                            0  \n",
       "WALKING_DOWNSTAIRS                 3  \n",
       "WALKING_UPSTAIRS                 452  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(features_test_df.activity_name.values,\n",
    "            y_test_hat,\n",
    "            rownames=['True'],\n",
    "            colnames=['Classified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-aerospace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
